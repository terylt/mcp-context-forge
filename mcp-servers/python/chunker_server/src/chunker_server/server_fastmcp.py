#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Location: ./mcp-servers/python/chunker_server/src/chunker_server/server_fastmcp.py
Copyright 2025
SPDX-License-Identifier: Apache-2.0
Authors: Mihai Criveti

Chunker FastMCP Server

Advanced text chunking and splitting server with multiple strategies using FastMCP framework.
Supports semantic chunking, recursive splitting, markdown-aware chunking, and more.
"""

import logging
import re
import sys
from typing import Any, Dict, List, Optional
from uuid import uuid4

from fastmcp import FastMCP
from pydantic import BaseModel, Field

# Configure logging to stderr to avoid MCP protocol interference
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stderr)],
)
logger = logging.getLogger(__name__)

# Create FastMCP server instance
mcp = FastMCP(
    name="chunker-server",
    version="2.0.0"
)


class TextChunker:
    """Advanced text chunking with multiple strategies."""

    def __init__(self):
        """Initialize the chunker."""
        self.available_strategies = self._check_available_strategies()

    def _check_available_strategies(self) -> Dict[str, bool]:
        """Check which chunking libraries are available."""
        strategies = {}

        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
            strategies['langchain'] = True
        except ImportError:
            strategies['langchain'] = False

        try:
            import nltk
            strategies['nltk'] = True
        except ImportError:
            strategies['nltk'] = False

        try:
            import spacy
            strategies['spacy'] = True
        except ImportError:
            strategies['spacy'] = False

        strategies['basic'] = True  # Always available

        return strategies

    def recursive_chunk(
        self,
        text: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        separators: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Recursive character-based chunking."""
        try:
            if self.available_strategies.get('langchain'):
                from langchain_text_splitters import RecursiveCharacterTextSplitter

                if separators is None:
                    separators = ["\n\n", "\n", ". ", " ", ""]

                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    separators=separators,
                    length_function=len,
                    is_separator_regex=False
                )

                chunks = splitter.split_text(text)
            else:
                # Fallback to basic implementation
                chunks = self._basic_recursive_chunk(text, chunk_size, chunk_overlap, separators)

            return {
                "success": True,
                "strategy": "recursive",
                "chunks": chunks,
                "chunk_count": len(chunks),
                "total_length": sum(len(chunk) for chunk in chunks),
                "average_chunk_size": sum(len(chunk) for chunk in chunks) / len(chunks) if chunks else 0
            }

        except Exception as e:
            logger.error(f"Error in recursive chunking: {e}")
            return {"success": False, "error": str(e)}

    def _basic_recursive_chunk(
        self,
        text: str,
        chunk_size: int,
        chunk_overlap: int,
        separators: Optional[List[str]] = None
    ) -> List[str]:
        """Basic recursive chunking implementation."""
        if separators is None:
            separators = ["\n\n", "\n", ". ", " "]

        def split_text_recursive(text: str, separators: List[str]) -> List[str]:
            if not separators or len(text) <= chunk_size:
                return [text] if text else []

            separator = separators[0]
            remaining_separators = separators[1:]

            parts = text.split(separator)
            chunks = []
            current_chunk = ""

            for part in parts:
                test_chunk = current_chunk + (separator if current_chunk else "") + part

                if len(test_chunk) <= chunk_size:
                    current_chunk = test_chunk
                else:
                    if current_chunk:
                        chunks.append(current_chunk)

                    if len(part) > chunk_size:
                        # Recursively split large parts
                        sub_chunks = split_text_recursive(part, remaining_separators)
                        chunks.extend(sub_chunks)
                        current_chunk = ""
                    else:
                        current_chunk = part

            if current_chunk:
                chunks.append(current_chunk)

            return chunks

        chunks = split_text_recursive(text, separators)

        # Add overlap if specified
        if chunk_overlap > 0 and len(chunks) > 1:
            overlapped_chunks = []
            for i, chunk in enumerate(chunks):
                if i == 0:
                    overlapped_chunks.append(chunk)
                else:
                    # Add overlap from previous chunk
                    prev_chunk = chunks[i - 1]
                    overlap_text = prev_chunk[-chunk_overlap:] if len(prev_chunk) > chunk_overlap else prev_chunk
                    overlapped_chunks.append(overlap_text + " " + chunk)

            return overlapped_chunks

        return chunks

    def markdown_chunk(
        self,
        text: str,
        headers_to_split_on: List[str] = ["#", "##", "###"],
        chunk_size: int = 1000,
        chunk_overlap: int = 100
    ) -> Dict[str, Any]:
        """Markdown-aware chunking that respects header structure."""
        try:
            if self.available_strategies.get('langchain'):
                from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

                # First split by headers
                headers = [(header, header) for header in headers_to_split_on]
                header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers)
                header_chunks = header_splitter.split_text(text)

                # Then split large chunks further
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )

                final_chunks = []
                for doc in header_chunks:
                    if len(doc.page_content) > chunk_size:
                        sub_chunks = text_splitter.split_text(doc.page_content)
                        for sub_chunk in sub_chunks:
                            final_chunks.append({
                                "content": sub_chunk,
                                "metadata": doc.metadata
                            })
                    else:
                        final_chunks.append({
                            "content": doc.page_content,
                            "metadata": doc.metadata
                        })

                chunks = [chunk["content"] for chunk in final_chunks]
                metadata = [chunk["metadata"] for chunk in final_chunks]

            else:
                # Basic markdown chunking
                chunks, metadata = self._basic_markdown_chunk(text, headers_to_split_on, chunk_size)

            return {
                "success": True,
                "strategy": "markdown",
                "chunks": chunks,
                "metadata": metadata,
                "chunk_count": len(chunks),
                "headers_used": headers_to_split_on
            }

        except Exception as e:
            logger.error(f"Error in markdown chunking: {e}")
            return {"success": False, "error": str(e)}

    def _basic_markdown_chunk(self, text: str, headers: List[str], chunk_size: int) -> tuple[List[str], List[Dict]]:
        """Basic markdown chunking implementation."""
        sections = []
        current_section = ""
        current_headers = {}

        lines = text.split('\n')

        for line in lines:
            # Check if line is a header
            is_header = False
            for header in headers:
                if line.strip().startswith(header + ' '):
                    # Start new section
                    if current_section:
                        sections.append({
                            "content": current_section.strip(),
                            "headers": current_headers.copy()
                        })

                    current_section = line + '\n'
                    header_text = line.strip()[len(header):].strip()
                    current_headers[header] = header_text
                    is_header = True
                    break

            if not is_header:
                current_section += line + '\n'

        # Add final section
        if current_section:
            sections.append({
                "content": current_section.strip(),
                "headers": current_headers.copy()
            })

        # Split large sections further
        final_chunks = []
        final_metadata = []

        for section in sections:
            if len(section["content"]) > chunk_size:
                # Split large sections
                sub_chunks = self._basic_recursive_chunk(section["content"], chunk_size, 100)
                for sub_chunk in sub_chunks:
                    final_chunks.append(sub_chunk)
                    final_metadata.append(section["headers"])
            else:
                final_chunks.append(section["content"])
                final_metadata.append(section["headers"])

        return final_chunks, final_metadata

    def sentence_chunk(
        self,
        text: str,
        sentences_per_chunk: int = 5,
        overlap_sentences: int = 1
    ) -> Dict[str, Any]:
        """Sentence-based chunking."""
        try:
            # Basic sentence splitting (can be enhanced with NLTK)
            if self.available_strategies.get('nltk'):
                import nltk
                try:
                    nltk.data.find('tokenizers/punkt')
                except LookupError:
                    nltk.download('punkt', quiet=True)

                sentences = nltk.sent_tokenize(text)
            else:
                # Basic sentence splitting with regex
                sentences = self._basic_sentence_split(text)

            chunks = []
            for i in range(0, len(sentences), sentences_per_chunk - overlap_sentences):
                chunk_sentences = sentences[i:i + sentences_per_chunk]
                chunk = ' '.join(chunk_sentences)
                chunks.append(chunk)

                # Stop if we've reached the end
                if i + sentences_per_chunk >= len(sentences):
                    break

            return {
                "success": True,
                "strategy": "sentence",
                "chunks": chunks,
                "chunk_count": len(chunks),
                "total_sentences": len(sentences),
                "sentences_per_chunk": sentences_per_chunk
            }

        except Exception as e:
            logger.error(f"Error in sentence chunking: {e}")
            return {"success": False, "error": str(e)}

    def _basic_sentence_split(self, text: str) -> List[str]:
        """Basic sentence splitting using regex."""
        # Split on sentence endings
        sentences = re.split(r'[.!?]+\s+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences

    def fixed_size_chunk(
        self,
        text: str,
        chunk_size: int = 1000,
        overlap: int = 0,
        split_on_word_boundary: bool = True
    ) -> Dict[str, Any]:
        """Fixed-size chunking with optional word boundary preservation."""
        try:
            chunks = []
            start = 0

            while start < len(text):
                end = start + chunk_size

                if end >= len(text):
                    # Last chunk
                    chunk = text[start:]
                    if chunk.strip():
                        chunks.append(chunk)
                    break

                chunk = text[start:end]

                # Adjust to word boundary if requested
                if split_on_word_boundary and end < len(text):
                    # Find last space within chunk
                    last_space = chunk.rfind(' ')
                    if last_space > chunk_size * 0.8:  # Don't go too far back
                        chunk = chunk[:last_space]
                        end = start + last_space

                chunks.append(chunk)
                start = end - overlap

            return {
                "success": True,
                "strategy": "fixed_size",
                "chunks": chunks,
                "chunk_count": len(chunks),
                "chunk_size": chunk_size,
                "overlap": overlap
            }

        except Exception as e:
            logger.error(f"Error in fixed-size chunking: {e}")
            return {"success": False, "error": str(e)}

    def semantic_chunk(
        self,
        text: str,
        min_chunk_size: int = 200,
        max_chunk_size: int = 2000,
        similarity_threshold: float = 0.8
    ) -> Dict[str, Any]:
        """Semantic chunking based on content similarity."""
        try:
            # For now, implement a simple semantic chunking based on paragraphs
            # This can be enhanced with embeddings and similarity measures

            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]

            chunks = []
            current_chunk = ""

            for paragraph in paragraphs:
                test_chunk = current_chunk + ("\n\n" if current_chunk else "") + paragraph

                if len(test_chunk) <= max_chunk_size:
                    current_chunk = test_chunk
                elif len(current_chunk) >= min_chunk_size:
                    chunks.append(current_chunk)
                    current_chunk = paragraph
                else:
                    # Current chunk too small, but adding would make it too big
                    if len(paragraph) > max_chunk_size:
                        # Split the large paragraph
                        if current_chunk:
                            chunks.append(current_chunk)
                        sub_chunks = self._split_large_text(paragraph, max_chunk_size, min_chunk_size)
                        chunks.extend(sub_chunks)
                        current_chunk = ""
                    else:
                        current_chunk = test_chunk

            if current_chunk:
                chunks.append(current_chunk)

            return {
                "success": True,
                "strategy": "semantic",
                "chunks": chunks,
                "chunk_count": len(chunks),
                "min_chunk_size": min_chunk_size,
                "max_chunk_size": max_chunk_size,
                "average_chunk_size": sum(len(chunk) for chunk in chunks) / len(chunks) if chunks else 0
            }

        except Exception as e:
            logger.error(f"Error in semantic chunking: {e}")
            return {"success": False, "error": str(e)}

    def _split_large_text(self, text: str, max_size: int, min_size: int) -> List[str]:
        """Split large text into smaller chunks."""
        chunks = []
        words = text.split()
        current_chunk = ""

        for word in words:
            test_chunk = current_chunk + (" " if current_chunk else "") + word

            if len(test_chunk) <= max_size:
                current_chunk = test_chunk
            else:
                if len(current_chunk) >= min_size:
                    chunks.append(current_chunk)
                    current_chunk = word
                else:
                    current_chunk = test_chunk  # Keep growing if below minimum

        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    def analyze_text(self, text: str) -> Dict[str, Any]:
        """Analyze text to recommend optimal chunking strategy."""
        try:
            analysis = {
                "total_length": len(text),
                "line_count": len(text.split('\n')),
                "paragraph_count": len([p for p in text.split('\n\n') if p.strip()]),
                "word_count": len(text.split()),
                "has_markdown_headers": bool(re.search(r'^#+\s', text, re.MULTILINE)),
                "has_numbered_sections": bool(re.search(r'^\d+\.', text, re.MULTILINE)),
                "has_bullet_points": bool(re.search(r'^[\*\-\+]\s', text, re.MULTILINE)),
                "average_paragraph_length": 0,
                "average_sentence_length": 0
            }

            # Calculate average paragraph length
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            if paragraphs:
                analysis["average_paragraph_length"] = sum(len(p) for p in paragraphs) / len(paragraphs)

            # Calculate average sentence length (basic)
            sentences = self._basic_sentence_split(text)
            if sentences:
                analysis["average_sentence_length"] = sum(len(s) for s in sentences) / len(sentences)

            # Recommend chunking strategy
            recommendations = []

            if analysis["has_markdown_headers"]:
                recommendations.append({
                    "strategy": "markdown",
                    "reason": "Text contains markdown headers - use markdown-aware chunking",
                    "suggested_params": {
                        "headers_to_split_on": ["#", "##", "###"],
                        "chunk_size": 1500
                    }
                })

            if analysis["average_paragraph_length"] > 500:
                recommendations.append({
                    "strategy": "semantic",
                    "reason": "Large paragraphs detected - semantic chunking recommended",
                    "suggested_params": {
                        "min_chunk_size": 300,
                        "max_chunk_size": 2000
                    }
                })

            if analysis["total_length"] > 10000:
                recommendations.append({
                    "strategy": "recursive",
                    "reason": "Large document - recursive chunking with overlap recommended",
                    "suggested_params": {
                        "chunk_size": 1000,
                        "chunk_overlap": 200
                    }
                })

            if not recommendations:
                recommendations.append({
                    "strategy": "fixed_size",
                    "reason": "Standard text - fixed-size chunking suitable",
                    "suggested_params": {
                        "chunk_size": 1000,
                        "split_on_word_boundary": True
                    }
                })

            analysis["recommendations"] = recommendations

            return {
                "success": True,
                "analysis": analysis
            }

        except Exception as e:
            logger.error(f"Error analyzing text: {e}")
            return {"success": False, "error": str(e)}

    def get_chunking_strategies(self) -> Dict[str, Any]:
        """Get available chunking strategies and their capabilities."""
        return {
            "available_strategies": self.available_strategies,
            "strategies": {
                "recursive": {
                    "description": "Hierarchical splitting with multiple separators",
                    "best_for": "General text, mixed content",
                    "parameters": ["chunk_size", "chunk_overlap", "separators"],
                    "available": self.available_strategies.get('langchain', True)
                },
                "markdown": {
                    "description": "Header-aware chunking for markdown documents",
                    "best_for": "Markdown documents, structured content",
                    "parameters": ["headers_to_split_on", "chunk_size", "chunk_overlap"],
                    "available": self.available_strategies.get('langchain', True)
                },
                "semantic": {
                    "description": "Content-aware chunking based on semantic boundaries",
                    "best_for": "Articles, essays, narrative text",
                    "parameters": ["min_chunk_size", "max_chunk_size", "similarity_threshold"],
                    "available": True
                },
                "sentence": {
                    "description": "Sentence-based chunking with overlap",
                    "best_for": "Precise sentence-level processing",
                    "parameters": ["sentences_per_chunk", "overlap_sentences"],
                    "available": True
                },
                "fixed_size": {
                    "description": "Fixed character count chunking",
                    "best_for": "Uniform chunk sizes, simple splitting",
                    "parameters": ["chunk_size", "overlap", "split_on_word_boundary"],
                    "available": True
                }
            },
            "libraries": {
                "langchain": self.available_strategies.get('langchain', False),
                "nltk": self.available_strategies.get('nltk', False),
                "spacy": self.available_strategies.get('spacy', False)
            }
        }


# Initialize chunker
chunker = TextChunker()


# Tool definitions using FastMCP
@mcp.tool(
    description="Chunk text using various strategies (recursive, semantic, sentence, fixed_size)"
)
async def chunk_text(
    text: str = Field(..., description="Text to chunk"),
    chunk_size: int = Field(1000, ge=100, le=100000, description="Maximum chunk size in characters"),
    chunk_overlap: int = Field(200, ge=0, description="Overlap between chunks in characters"),
    chunking_strategy: str = Field("recursive", pattern="^(recursive|semantic|sentence|fixed_size)$",
                                   description="Chunking strategy to use"),
    separators: Optional[List[str]] = Field(None, description="Custom separators for splitting"),
    preserve_structure: bool = Field(True, description="Preserve document structure when possible")
) -> Dict[str, Any]:
    """Chunk text using the specified strategy."""

    if chunking_strategy == "recursive":
        return chunker.recursive_chunk(
            text=text,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=separators
        )
    elif chunking_strategy == "semantic":
        return chunker.semantic_chunk(
            text=text,
            max_chunk_size=chunk_size
        )
    elif chunking_strategy == "sentence":
        return chunker.sentence_chunk(text=text)
    elif chunking_strategy == "fixed_size":
        return chunker.fixed_size_chunk(
            text=text,
            chunk_size=chunk_size,
            overlap=chunk_overlap
        )
    else:
        return {"success": False, "error": f"Unknown strategy: {chunking_strategy}"}


@mcp.tool(
    description="Chunk markdown text with header awareness"
)
async def chunk_markdown(
    text: str = Field(..., description="Markdown text to chunk"),
    headers_to_split_on: List[str] = Field(["#", "##", "###"], description="Headers to split on"),
    chunk_size: int = Field(1000, ge=100, le=100000, description="Maximum chunk size"),
    chunk_overlap: int = Field(100, ge=0, description="Overlap between chunks")
) -> Dict[str, Any]:
    """Chunk markdown text with awareness of header structure."""
    return chunker.markdown_chunk(
        text=text,
        headers_to_split_on=headers_to_split_on,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )


@mcp.tool(
    description="Semantic chunking based on content similarity"
)
async def semantic_chunk(
    text: str = Field(..., description="Text to chunk semantically"),
    min_chunk_size: int = Field(200, ge=50, description="Minimum chunk size"),
    max_chunk_size: int = Field(2000, ge=100, le=100000, description="Maximum chunk size"),
    similarity_threshold: float = Field(0.8, ge=0.0, le=1.0, description="Similarity threshold for grouping")
) -> Dict[str, Any]:
    """Perform semantic chunking based on content boundaries."""
    return chunker.semantic_chunk(
        text=text,
        min_chunk_size=min_chunk_size,
        max_chunk_size=max_chunk_size,
        similarity_threshold=similarity_threshold
    )


@mcp.tool(
    description="Sentence-based chunking with configurable grouping"
)
async def sentence_chunk(
    text: str = Field(..., description="Text to chunk by sentences"),
    sentences_per_chunk: int = Field(5, ge=1, le=50, description="Target sentences per chunk"),
    overlap_sentences: int = Field(1, ge=0, le=10, description="Overlapping sentences between chunks")
) -> Dict[str, Any]:
    """Chunk text by grouping sentences."""
    return chunker.sentence_chunk(
        text=text,
        sentences_per_chunk=sentences_per_chunk,
        overlap_sentences=overlap_sentences
    )


@mcp.tool(
    description="Fixed-size chunking with word boundary options"
)
async def fixed_size_chunk(
    text: str = Field(..., description="Text to chunk"),
    chunk_size: int = Field(1000, ge=100, le=100000, description="Fixed chunk size in characters"),
    overlap: int = Field(0, ge=0, description="Overlap between chunks"),
    split_on_word_boundary: bool = Field(True, description="Split on word boundaries to avoid breaking words")
) -> Dict[str, Any]:
    """Chunk text into fixed-size pieces."""
    return chunker.fixed_size_chunk(
        text=text,
        chunk_size=chunk_size,
        overlap=overlap,
        split_on_word_boundary=split_on_word_boundary
    )


@mcp.tool(
    description="Analyze text and recommend optimal chunking strategy"
)
async def analyze_text(
    text: str = Field(..., description="Text to analyze for chunking recommendations")
) -> Dict[str, Any]:
    """Analyze text characteristics and recommend optimal chunking strategy."""
    return chunker.analyze_text(text)


@mcp.tool(
    description="List available chunking strategies and capabilities"
)
async def get_strategies() -> Dict[str, Any]:
    """Get information about available chunking strategies and libraries."""
    return chunker.get_chunking_strategies()


def main():
    """Main server entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="Chunker FastMCP Server")
    parser.add_argument("--transport", choices=["stdio", "http"], default="stdio",
                        help="Transport mode (stdio or http)")
    parser.add_argument("--host", default="0.0.0.0", help="HTTP host")
    parser.add_argument("--port", type=int, default=9001, help="HTTP port")

    args = parser.parse_args()

    if args.transport == "http":
        logger.info(f"Starting Chunker FastMCP Server on HTTP at {args.host}:{args.port}")
        mcp.run(transport="http", host=args.host, port=args.port)
    else:
        logger.info("Starting Chunker FastMCP Server on stdio")
        mcp.run()


if __name__ == "__main__":
    main()
